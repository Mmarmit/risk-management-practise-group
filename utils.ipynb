{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead262ce-f700-4eb5-b7e3-66a1c38cdc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def univ(df,remlist=list()):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    out=df.select_dtypes(include=np.number).columns\n",
    "    univlist=list(set(out)-set(remlist))\n",
    "    df_numeric=df[univlist]\n",
    "    des1=df_numeric.describe(percentiles=[0.0,0.005,0.01,0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8,0.9,0.99,0.999,1],include = 'all').T\n",
    "    des2 =((df_numeric.isnull().sum())*100/df_numeric.shape[0]).to_frame(name ='missing')\n",
    "    df1=pd.concat([des1, des2],axis=1)\n",
    "    df2=df1.drop('count', axis=1)\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b601bc7-6fec-40d7-b84b-025733270fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def KS_table(actual, pred , return_ks_table = True,num_deciles=10):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    data = pd.DataFrame({'target':actual,'prob':pred})\n",
    "    data['target0'] = 1 - data['target']\n",
    "    q   = data['prob'].quantile(np.linspace(0,1,num_deciles+1))\n",
    "    c   = np.unique(q)\n",
    "    data['bucket'] =  pd.cut(data['prob'],c,include_lowest=True,labels=range(1,len(c)))\n",
    "    grouped = data.groupby('bucket', as_index = False)\n",
    "    kstable = pd.DataFrame()\n",
    "    kstable['min_prob'] = grouped.min()['prob']\n",
    "    kstable['max_prob'] = grouped.max()['prob']\n",
    "    kstable['avg_prob'] = grouped.mean()['prob']\n",
    "    kstable['events']=grouped.sum()['target']\n",
    "    kstable['nonevents'] = grouped.sum()['target0']\n",
    "    kstable = kstable.sort_values(by=\"min_prob\", ascending=False).reset_index(drop = True)\n",
    "    kstable['event_rate'] = (kstable.events / data['target'].sum()).apply('{0:.2%}'.format)\n",
    "    kstable['nonevent_rate'] = (kstable.nonevents / data['target0'].sum()).apply('{0:.2%}'.format)\n",
    "    kstable['cum_eventrate']=(kstable.events / data['target'].sum()).cumsum()\n",
    "    kstable['cum_noneventrate']=(kstable.nonevents / data['target0'].sum()).cumsum()\n",
    "    kstable['bad_rate']=kstable.events*100/(kstable.events+kstable.nonevents)\n",
    "    kstable['KS'] = np.round(kstable['cum_eventrate']-kstable['cum_noneventrate'], 5) * 100\n",
    "    #Formating\n",
    "    kstable['cum_eventrate']= kstable['cum_eventrate'].apply('{0:.2%}'.format)\n",
    "    kstable['cum_noneventrate']= kstable['cum_noneventrate'].apply('{0:.2%}'.format)\n",
    "    kstable.index = range(num_deciles,0,-1)\n",
    "    kstable.index.rename('Decile', inplace=True)\n",
    "    kstable.reset_index(inplace = True)\n",
    "    kstable.sort_values('Decile', ascending = True, inplace = True)\n",
    "    kstable.reset_index(drop = True, inplace = True)\n",
    "    #Display KS\n",
    "    from colorama import Fore\n",
    "    if return_ks_table :\n",
    "        print(Fore.RED + \"KS is \" + str(round(max(kstable['KS']),2))+\"%\"+ \" at decile \" + str((kstable[kstable['KS']==max(kstable['KS'])]['Decile'].values)))\n",
    "        return(kstable)\n",
    "    else :\n",
    "        return round(max(kstable['KS']),2)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc61336-5af7-44ac-a2a0-b6ec54b6fb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iv_woe(data, target, bins=10, show_woe=False):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    #Empty Dataframe\n",
    "    newDF,woeDF = pd.DataFrame(), pd.DataFrame()\n",
    "    \n",
    "    #Extract Column Names\n",
    "    cols = data.columns\n",
    "    \n",
    "    #Run WOE and IV on all the independent variables\n",
    "    for ivars in cols[~cols.isin([target])]:\n",
    "        if (data[ivars].dtype.kind in 'bifc') and (len(np.unique(data[ivars]))>10):\n",
    "            binned_x = pd.qcut(data[ivars], bins,  duplicates='drop')\n",
    "            d0 = pd.DataFrame({'x': binned_x, 'y': data[target]})\n",
    "        else:\n",
    "            d0 = pd.DataFrame({'x': data[ivars], 'y': data[target]})\n",
    "        d0 = d0.astype({\"x\": str})\n",
    "        d = d0.groupby(\"x\", as_index=False, dropna=False).agg({\"y\": [\"count\", \"sum\"]})\n",
    "        d.columns = ['Cutoff', 'N', 'Events']\n",
    "        d['% of Events'] = np.maximum(d['Events'], 0.5) / d['Events'].sum()\n",
    "        d['Non-Events'] = d['N'] - d['Events']\n",
    "        d['% of Non-Events'] = np.maximum(d['Non-Events'], 0.5) / d['Non-Events'].sum()\n",
    "        d['WoE'] = np.log(d['% of Non-Events']/d['% of Events'])\n",
    "        d['IV'] = d['WoE'] * (d['% of Non-Events']-d['% of Events'])\n",
    "        d.insert(loc=0, column='Variable', value=ivars)\n",
    "        #print(\"Information value of \" + ivars + \" is \" + str(round(d['IV'].sum(),6)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3be455-7a0d-49a0-8fb9-3c9946c99631",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_psi_continuous(dataframe1, dataframe2,features, bins=10):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    PSI = pd.DataFrame(columns = ['Feature','Value'])  \n",
    "    cols = features\n",
    "    for i in cols:\n",
    "        # Divide the range of the variable into equal-width bins\n",
    "        expected = dataframe1[i]\n",
    "        actual = dataframe2[i]\n",
    "        breakpoints = np.linspace(min(expected), max(expected), bins+1)\n",
    "        # Calculate the observed and expected frequencies for each bin\n",
    "        expected_hist, _ = np.histogram(expected, bins=breakpoints)\n",
    "        actual_hist, _ = np.histogram(actual, bins=breakpoints)\n",
    "        # Calculate the proportions within each bin\n",
    "        expected_prop = expected_hist / len(expected)\n",
    "        actual_prop = actual_hist / len(actual)\n",
    "        # Avoid zero numerator or denominator errors\n",
    "        epsilon = 1e-10\n",
    "        actual_prop[actual_prop == 0] = epsilon\n",
    "        expected_prop[expected_prop == 0] = epsilon\n",
    "        # Calculate the PSI value\n",
    "        psi = np.sum((actual_prop - expected_prop) * np.log(actual_prop / expected_prop))\n",
    "        temp = pd.DataFrame([{'Feature': i, 'Value': psi}])\n",
    "        PSI= pd.concat([temp, PSI])\n",
    "    return PSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07980acd-bd3f-4ab7-9254-98e5bbede316",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_psi_categorical(dataframe1, dataframe2,features):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    PSI = pd.DataFrame(columns = ['Feature','Value'])  \n",
    "    cols = features\n",
    "    for i in cols:\n",
    "        expected = dataframe1[i]\n",
    "        actual = dataframe2[i]\n",
    "    # Create frequency tables for expected and actual values\n",
    "        expected_freq = pd.Series(expected).value_counts(normalize=True)\n",
    "        actual_freq = pd.Series(actual).value_counts(normalize=True)\n",
    "\n",
    "        # Merge the frequency tables\n",
    "        freq_table = pd.concat([expected_freq, actual_freq], axis=1, keys=['Expected', 'Actual']).fillna(0)\n",
    "        freq_table[freq_table['Actual'] == 0] = 0.000001\n",
    "        freq_table[freq_table['Expected'] == 0] = 0.000000001\n",
    "        # Calculate the PSI value\n",
    "        psi = np.sum((freq_table['Actual'] - freq_table['Expected']) * np.log(freq_table['Actual'] / freq_table['Expected']))\n",
    "        temp = pd.DataFrame([{'Feature': i, 'Value': psi}])\n",
    "        PSI= pd.concat([temp, PSI])\n",
    "    return PSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf5f74e-5c60-41c4-bedc-19b3c067da12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bivariate(data,cols,target_var,bins=10):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    k=1\n",
    "    for i in cols:\n",
    "        var = data[i].values\n",
    "        q   = np.nanquantile(var,np.linspace(0,1,bins+1))\n",
    "        c   = list(np.unique(q))\n",
    "        c1  = [-np.inf]+c[1:-1]+[np.inf]\n",
    "        data_cut = pd.cut(var,c1,include_lowest=True,labels=range(1,len(c1)))\n",
    "        data_cut1=np.where(data_cut.isna(),\"missing\",data_cut)\n",
    "        tar_data = data[target_var].values\n",
    "        df = pd.DataFrame({'bin':data_cut1,'value':data[i].values,'target':tar_data})\n",
    "        records = df.shape[0]\n",
    "        df_grp=df.groupby('bin').agg(\n",
    "        total=('target','count'),\n",
    "        events=('target','sum'),\n",
    "        min_value = ('value','min'),\n",
    "        max_value = ('value','max'),\n",
    "        ).reset_index()\n",
    "        df_grp['variable']=i\n",
    "        df_grp['event_rate']=df_grp['events']/df_grp['total']\n",
    "        df_grp['non_events']=df_grp['total'] - df_grp['events']\n",
    "        df_grp['non_events_perc']= (df_grp['non_events'])/records\n",
    "        df_grp['events_perc']= (df_grp['events'])/records\n",
    "        df_grp['pop_perc']= (df_grp['total'])/records\n",
    "        df_bivar = df_grp[[\"variable\",\"bin\",\"min_value\",\"max_value\",\"total\",\"events\",\"events_perc\",\"non_events\",\"non_events_perc\",\"pop_perc\",\"event_rate\"]]    \n",
    "        if k==1:\n",
    "            data_out=df_bivar.copy()\n",
    "        else:\n",
    "            data_out=pd.concat([data_out,df_bivar],axis=0).copy()\n",
    "        k = k+1\n",
    "    return data_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8263d6e-c367-4a8f-98c8-336d06f51485",
   "metadata": {},
   "outputs": [],
   "source": [
    "def woe_bivariate(train,test,target_var,cols,bins=10):\n",
    "    def cal_woe(df,var1):\n",
    "        records = df.shape[0]\n",
    "        df_grp=df.groupby('dec').agg(\n",
    "        total=('target','count'),\n",
    "        events=('target','sum'),\n",
    "        min_value = ('value','min'),\n",
    "        max_value = ('value','max'),\n",
    "        ).reset_index()\n",
    "        df_grp['variable']=var1\n",
    "        df_grp['event_rate']=df_grp['events']/df_grp['total']\n",
    "        df_grp['non_events']=df_grp['total'] - df_grp['events']\n",
    "        df_grp['non_events_perc']= (df_grp['non_events']*100)/records\n",
    "        df_grp['events_perc']= (df_grp['events']*100)/records\n",
    "        df_grp['pop_perc']= (df_grp['total']*100)/records\n",
    "        df_grp['neg_per']=df_grp['non_events']/df_grp['non_events'].sum()\n",
    "        df_grp['pos_per']=df_grp['events']/df_grp['events'].sum()\n",
    "        df_grp['woe']=np.log((df_grp['pos_per']+1e-08)/(df_grp['neg_per']+1e-08))\n",
    "        df_grp_formatted = df_grp[[\"variable\",\"dec\",\"min_value\",\"max_value\",\"total\",\"pop_perc\",\"events\",\"events_perc\",\"non_events\",\"non_events_perc\",\"event_rate\",\"woe\"]]\n",
    "        return df_grp_formatted\n",
    "        return dict(zip(df_grp['var'],df_grp['woe']))\n",
    "    k=1\n",
    "    for i in cols:\n",
    "        var = train[i].values\n",
    "        q   = np.nanquantile(var,np.linspace(0,1,bins+1))\n",
    "        c   = list(np.unique(q))\n",
    "        c1  = [-np.inf]+c[1:-1]+[np.inf]\n",
    "        train_cut = pd.cut(var,c1,include_lowest=True,labels=range(1,len(c1)))\n",
    "        train_cut1=np.where(train_cut.isna(),\"missing\",train_cut)\n",
    "        test_cut = pd.cut(test[i].values,c1,include_lowest=True,labels=range(1,len(c1)))\n",
    "        test_cut1=np.where(test_cut.isna(),\"missing\",test_cut)\n",
    "        tar_train = train[target_var].values\n",
    "        tar_test = test[target_var].values\n",
    "        df_tr = pd.DataFrame({'dec':train_cut1,'value':train[i].values,'target':tar_train})\n",
    "        df_te = pd.DataFrame({'dec':test_cut1,'value':test[i].values,'target':tar_test})\n",
    "        tr_a = cal_woe(df_tr,i).copy()\n",
    "        te_a = cal_woe(df_te,i).copy()\n",
    "        if k==1:\n",
    "            train_out=tr_a.copy()\n",
    "            test_out=te_a.copy()\n",
    "        else:\n",
    "            train_out=pd.concat([train_out,tr_a],axis=0).copy()\n",
    "            test_out=pd.concat([test_out,te_a],axis=0).copy()\n",
    "        k = k+1\n",
    "    return train_out,test_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e06f37-fa4c-4b83-91d6-45a1029ced2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_perf(train_ks_df,scored_df,target_col,num_deciles=10):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    Y_val=scored_df[scored_df[target_col].isin([0,1])==True][target_col]\n",
    "    pred_prob_val=scored_df[scored_df[target_col].isin([0,1])==True]['pred_prob']\n",
    "    train_ks_df.columns=[col+'_train' for col in train_ks_df.columns]\n",
    "    pred_prob_val_df = pd.DataFrame({'target_val':Y_val,'prob_val':pred_prob_val})\n",
    "    pred_prob_val_df['target0_val'] = 1 - pred_prob_val_df['target_val']\n",
    "    train_ks_val=train_ks_df.merge(pred_prob_val_df,how='cross')\n",
    "    train_ks_val1=train_ks_val[(train_ks_val['prob_val']>=train_ks_val['min_prob_train']) & (train_ks_val['prob_val'] <=train_ks_val['max_prob_train']) ].sort_values('Decile_train')\n",
    "    train_ks_val2=train_ks_val1.groupby(['Decile_train','min_prob_train','max_prob_train']).agg(avg_prob=('prob_val','mean'),num_cust=('prob_val','count'),events=('target_val','sum'),nonevents=('target0_val','sum')).reset_index()\n",
    "    train_ks_val2.sort_values('Decile_train', ascending = True, inplace = True)\n",
    "    train_ks_val2['event_rate'] = (train_ks_val2.events / pred_prob_val_df['target_val'].sum()).apply('{0:.2%}'.format)\n",
    "    train_ks_val2['nonevent_rate'] = (train_ks_val2.nonevents / pred_prob_val_df['target0_val'].sum()).apply('{0:.2%}'.format)\n",
    "    train_ks_val2['cum_eventrate']=(train_ks_val2.events / pred_prob_val_df['target_val'].sum()).cumsum()\n",
    "    train_ks_val2['cum_noneventrate']=(train_ks_val2.nonevents / pred_prob_val_df['target0_val'].sum()).cumsum()\n",
    "    train_ks_val2['bad_rate']=train_ks_val2.events*100/(train_ks_val2.events+train_ks_val2.nonevents)\n",
    "    train_ks_val2['KS'] = abs(np.round(train_ks_val2['cum_eventrate']-train_ks_val2['cum_noneventrate'], 5) * 100)\n",
    "    train_ks_val2['population_percent'] = np.round(train_ks_val2['num_cust']/train_ks_val2['num_cust'].sum(), 5) * 100\n",
    "    print(\"% cust in low risk \",int(num_deciles/2),\" Decile :\",train_ks_val2[train_ks_val2['Decile_train']<(num_deciles/2 + 1)]['num_cust'].sum()/train_ks_val2['num_cust'].sum())\n",
    "    train_ks_val3 = train_ks_val2[['Decile_train','min_prob_train','max_prob_train','avg_prob','num_cust','event_rate','events','nonevents','cum_eventrate','cum_noneventrate','bad_rate','KS', 'population_percent']].sort_values('Decile_train', ascending = True)\n",
    "    return train_ks_val3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
